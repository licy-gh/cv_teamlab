/root/miniconda3/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
/root/miniconda3/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
Use GPU: 1 for training
Use GPU: 0 for training
INFO 2022-05-07 17:18:05 distributed_c10d.py: 187] Added key: store_based_barrier_key:1 to store for rank: 1
INFO 2022-05-07 17:18:05 distributed_c10d.py: 187] Added key: store_based_barrier_key:1 to store for rank: 0
backbone param:
{'img_size': 224, 'patch_size': 4, 'in_chans': 3, 'embed_dim': 96, 'depths': [2, 2, 18, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'drop_rate': 0.0, 'drop_path_rate': 0.3}
head param:
{'feat_dim': 512, 'num_class': 72778, 'is_am': 1, 'margin': 0.35, 'mv_weight': 1.12, 'scale': 32}
backbone param:
{'img_size': 224, 'patch_size': 4, 'in_chans': 3, 'embed_dim': 96, 'depths': [2, 2, 18, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'drop_rate': 0.0, 'drop_path_rate': 0.3}
head param:
{'feat_dim': 512, 'num_class': 72778, 'is_am': 1, 'margin': 0.35, 'mv_weight': 1.12, 'scale': 32}
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
INFO 2022-05-07 17:18:14 train.py: 88] Epoch 0, iter 0/628, lr 0.000000, loss 16.129133
INFO 2022-05-07 17:18:14 distributed.py: 693] Reducer buckets have been rebuilt in this iteration.
INFO 2022-05-07 17:18:14 distributed.py: 693] Reducer buckets have been rebuilt in this iteration.
INFO 2022-05-07 17:24:09 train.py: 88] Epoch 0, iter 200/628, lr 0.000016, loss 16.014233
INFO 2022-05-07 17:30:05 train.py: 88] Epoch 0, iter 400/628, lr 0.000032, loss 14.868338
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
INFO 2022-05-07 17:35:59 train.py: 88] Epoch 0, iter 600/628, lr 0.000048, loss 13.127121
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
INFO 2022-05-07 17:36:47 train.py: 108] Save checkpoint Epoch_0.pt to disk...
INFO 2022-05-07 17:36:48 train.py: 88] Epoch 1, iter 0/628, lr 0.000050, loss 12.674397
INFO 2022-05-07 17:42:43 train.py: 88] Epoch 1, iter 200/628, lr 0.000066, loss 12.247855
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
INFO 2022-05-07 17:48:36 train.py: 88] Epoch 1, iter 400/628, lr 0.000082, loss 12.116854
INFO 2022-05-07 17:54:31 train.py: 88] Epoch 1, iter 600/628, lr 0.000098, loss 11.970573
INFO 2022-05-07 17:55:19 train.py: 108] Save checkpoint Epoch_1.pt to disk...
INFO 2022-05-07 17:55:20 train.py: 88] Epoch 2, iter 0/628, lr 0.000100, loss 11.596511
INFO 2022-05-07 18:01:14 train.py: 88] Epoch 2, iter 200/628, lr 0.000116, loss 11.618296
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
INFO 2022-05-07 18:07:07 train.py: 88] Epoch 2, iter 400/628, lr 0.000132, loss 11.741487
INFO 2022-05-07 18:13:01 train.py: 88] Epoch 2, iter 600/628, lr 0.000148, loss 11.873045
INFO 2022-05-07 18:13:48 train.py: 108] Save checkpoint Epoch_2.pt to disk...
INFO 2022-05-07 18:13:50 train.py: 88] Epoch 3, iter 0/628, lr 0.000150, loss 11.190286
INFO 2022-05-07 18:19:44 train.py: 88] Epoch 3, iter 200/628, lr 0.000166, loss 10.973143
INFO 2022-05-07 18:25:37 train.py: 88] Epoch 3, iter 400/628, lr 0.000182, loss 10.567161
INFO 2022-05-07 18:31:31 train.py: 88] Epoch 3, iter 600/628, lr 0.000198, loss 10.105028
INFO 2022-05-07 18:32:19 train.py: 108] Save checkpoint Epoch_3.pt to disk...
INFO 2022-05-07 18:32:20 train.py: 88] Epoch 4, iter 0/628, lr 0.000200, loss 9.643654
INFO 2022-05-07 18:38:14 train.py: 88] Epoch 4, iter 200/628, lr 0.000216, loss 9.556869
INFO 2022-05-07 18:44:07 train.py: 88] Epoch 4, iter 400/628, lr 0.000232, loss 8.942942
INFO 2022-05-07 18:50:00 train.py: 88] Epoch 4, iter 600/628, lr 0.000248, loss 8.588556
INFO 2022-05-07 18:50:47 train.py: 108] Save checkpoint Epoch_4.pt to disk...
INFO 2022-05-07 18:50:48 train.py: 88] Epoch 5, iter 0/628, lr 0.000250, loss 8.360959
INFO 2022-05-07 18:56:40 train.py: 88] Epoch 5, iter 200/628, lr 0.000266, loss 8.268609
INFO 2022-05-07 19:02:31 train.py: 88] Epoch 5, iter 400/628, lr 0.000282, loss 8.077896
INFO 2022-05-07 19:08:20 train.py: 88] Epoch 5, iter 600/628, lr 0.000298, loss 8.000559
INFO 2022-05-07 19:09:07 train.py: 108] Save checkpoint Epoch_5.pt to disk...
INFO 2022-05-07 19:09:09 train.py: 88] Epoch 6, iter 0/628, lr 0.000300, loss 8.011557
INFO 2022-05-07 19:14:58 train.py: 88] Epoch 6, iter 200/628, lr 0.000316, loss 8.071924
INFO 2022-05-07 19:20:46 train.py: 88] Epoch 6, iter 400/628, lr 0.000332, loss 8.142249
INFO 2022-05-07 19:26:34 train.py: 88] Epoch 6, iter 600/628, lr 0.000348, loss 8.231812
INFO 2022-05-07 19:27:21 train.py: 108] Save checkpoint Epoch_6.pt to disk...
INFO 2022-05-07 19:27:22 train.py: 88] Epoch 7, iter 0/628, lr 0.000350, loss 8.140138
INFO 2022-05-07 19:33:11 train.py: 88] Epoch 7, iter 200/628, lr 0.000366, loss 8.222857
INFO 2022-05-07 19:38:59 train.py: 88] Epoch 7, iter 400/628, lr 0.000382, loss 8.343630
INFO 2022-05-07 19:44:46 train.py: 88] Epoch 7, iter 600/628, lr 0.000398, loss 8.431416
INFO 2022-05-07 19:45:33 train.py: 108] Save checkpoint Epoch_7.pt to disk...
INFO 2022-05-07 19:45:34 train.py: 88] Epoch 8, iter 0/628, lr 0.000400, loss 8.461235
INFO 2022-05-07 19:51:21 train.py: 88] Epoch 8, iter 200/628, lr 0.000416, loss 8.309357
INFO 2022-05-07 19:57:08 train.py: 88] Epoch 8, iter 400/628, lr 0.000432, loss 8.261715
INFO 2022-05-07 20:02:54 train.py: 88] Epoch 8, iter 600/628, lr 0.000448, loss 8.189173
INFO 2022-05-07 20:03:41 train.py: 108] Save checkpoint Epoch_8.pt to disk...
INFO 2022-05-07 20:03:43 train.py: 88] Epoch 9, iter 0/628, lr 0.000450, loss 8.007356
INFO 2022-05-07 20:09:30 train.py: 88] Epoch 9, iter 200/628, lr 0.000466, loss 7.983619
INFO 2022-05-07 20:15:17 train.py: 88] Epoch 9, iter 400/628, lr 0.000482, loss 7.788191
INFO 2022-05-07 20:21:07 train.py: 88] Epoch 9, iter 600/628, lr 0.000498, loss 7.555281
INFO 2022-05-07 20:21:54 train.py: 108] Save checkpoint Epoch_9.pt to disk...
INFO 2022-05-07 20:21:55 train.py: 88] Epoch 10, iter 0/628, lr 0.000453, loss 7.456068
INFO 2022-05-07 20:27:46 train.py: 88] Epoch 10, iter 200/628, lr 0.000450, loss 7.154589
INFO 2022-05-07 20:33:37 train.py: 88] Epoch 10, iter 400/628, lr 0.000447, loss 6.840692
INFO 2022-05-07 20:39:29 train.py: 88] Epoch 10, iter 600/628, lr 0.000444, loss 6.396450
INFO 2022-05-07 20:40:16 train.py: 108] Save checkpoint Epoch_10.pt to disk...
INFO 2022-05-07 20:40:18 train.py: 88] Epoch 11, iter 0/628, lr 0.000443, loss 6.355036
INFO 2022-05-07 20:46:07 train.py: 88] Epoch 11, iter 200/628, lr 0.000440, loss 6.136161
INFO 2022-05-07 20:51:56 train.py: 88] Epoch 11, iter 400/628, lr 0.000437, loss 5.613625
INFO 2022-05-07 20:57:45 train.py: 88] Epoch 11, iter 600/628, lr 0.000433, loss 5.221649
INFO 2022-05-07 20:58:33 train.py: 108] Save checkpoint Epoch_11.pt to disk...
INFO 2022-05-07 20:58:34 train.py: 88] Epoch 12, iter 0/628, lr 0.000433, loss 5.077359
INFO 2022-05-07 21:04:25 train.py: 88] Epoch 12, iter 200/628, lr 0.000429, loss 4.848434
INFO 2022-05-07 21:10:15 train.py: 88] Epoch 12, iter 400/628, lr 0.000426, loss 4.423015
INFO 2022-05-07 21:16:05 train.py: 88] Epoch 12, iter 600/628, lr 0.000422, loss 3.945748
INFO 2022-05-07 21:16:52 train.py: 108] Save checkpoint Epoch_12.pt to disk...
INFO 2022-05-07 21:16:53 train.py: 88] Epoch 13, iter 0/628, lr 0.000422, loss 3.827648
INFO 2022-05-07 21:22:43 train.py: 88] Epoch 13, iter 200/628, lr 0.000418, loss 3.598366
INFO 2022-05-07 21:28:34 train.py: 88] Epoch 13, iter 400/628, lr 0.000415, loss 3.188099
INFO 2022-05-07 21:34:24 train.py: 88] Epoch 13, iter 600/628, lr 0.000411, loss 2.779724
INFO 2022-05-07 21:35:11 train.py: 108] Save checkpoint Epoch_13.pt to disk...
INFO 2022-05-07 21:35:12 train.py: 88] Epoch 14, iter 0/628, lr 0.000410, loss 2.593940
INFO 2022-05-07 21:41:02 train.py: 88] Epoch 14, iter 200/628, lr 0.000406, loss 2.529310
INFO 2022-05-07 21:46:52 train.py: 88] Epoch 14, iter 400/628, lr 0.000403, loss 2.198295
INFO 2022-05-07 21:52:42 train.py: 88] Epoch 14, iter 600/628, lr 0.000399, loss 1.886001
INFO 2022-05-07 21:53:29 train.py: 108] Save checkpoint Epoch_14.pt to disk...
INFO 2022-05-07 21:53:31 train.py: 88] Epoch 15, iter 0/628, lr 0.000398, loss 1.824368
INFO 2022-05-07 21:59:20 train.py: 88] Epoch 15, iter 200/628, lr 0.000394, loss 1.664349
INFO 2022-05-07 22:05:10 train.py: 88] Epoch 15, iter 400/628, lr 0.000390, loss 1.381046
INFO 2022-05-07 22:11:02 train.py: 88] Epoch 15, iter 600/628, lr 0.000386, loss 1.188780
INFO 2022-05-07 22:11:49 train.py: 108] Save checkpoint Epoch_15.pt to disk...
INFO 2022-05-07 22:11:51 train.py: 88] Epoch 16, iter 0/628, lr 0.000385, loss 0.967099
INFO 2022-05-07 22:17:40 train.py: 88] Epoch 16, iter 200/628, lr 0.000381, loss 1.000001
INFO 2022-05-07 22:23:29 train.py: 88] Epoch 16, iter 400/628, lr 0.000377, loss 0.843911
INFO 2022-05-07 22:29:20 train.py: 88] Epoch 16, iter 600/628, lr 0.000372, loss 0.740940
INFO 2022-05-07 22:30:07 train.py: 108] Save checkpoint Epoch_16.pt to disk...
INFO 2022-05-07 22:30:09 train.py: 88] Epoch 17, iter 0/628, lr 0.000372, loss 0.683027
INFO 2022-05-07 22:35:58 train.py: 88] Epoch 17, iter 200/628, lr 0.000367, loss 0.624966
INFO 2022-05-07 22:41:47 train.py: 88] Epoch 17, iter 400/628, lr 0.000363, loss 0.563921
INFO 2022-05-07 22:47:37 train.py: 88] Epoch 17, iter 600/628, lr 0.000359, loss 0.478155
INFO 2022-05-07 22:48:24 train.py: 108] Save checkpoint Epoch_17.pt to disk...
INFO 2022-05-07 22:48:25 train.py: 88] Epoch 18, iter 0/628, lr 0.000358, loss 0.469096
INFO 2022-05-07 22:54:16 train.py: 88] Epoch 18, iter 200/628, lr 0.000353, loss 0.404762
INFO 2022-05-07 23:00:05 train.py: 88] Epoch 18, iter 400/628, lr 0.000349, loss 0.342137
INFO 2022-05-07 23:05:54 train.py: 88] Epoch 18, iter 600/628, lr 0.000344, loss 0.320356
INFO 2022-05-07 23:06:41 train.py: 108] Save checkpoint Epoch_18.pt to disk...
INFO 2022-05-07 23:06:43 train.py: 88] Epoch 19, iter 0/628, lr 0.000344, loss 0.309691
INFO 2022-05-07 23:12:33 train.py: 88] Epoch 19, iter 200/628, lr 0.000339, loss 0.283878
INFO 2022-05-07 23:18:23 train.py: 88] Epoch 19, iter 400/628, lr 0.000334, loss 0.260446
INFO 2022-05-07 23:24:11 train.py: 88] Epoch 19, iter 600/628, lr 0.000330, loss 0.219770
INFO 2022-05-07 23:24:58 train.py: 108] Save checkpoint Epoch_19.pt to disk...
INFO 2022-05-07 23:25:00 train.py: 88] Epoch 20, iter 0/628, lr 0.000329, loss 0.187924
INFO 2022-05-07 23:30:49 train.py: 88] Epoch 20, iter 200/628, lr 0.000324, loss 0.193317
INFO 2022-05-07 23:36:38 train.py: 88] Epoch 20, iter 400/628, lr 0.000320, loss 0.182330
INFO 2022-05-07 23:42:24 train.py: 88] Epoch 20, iter 600/628, lr 0.000315, loss 0.167422
INFO 2022-05-07 23:43:11 train.py: 108] Save checkpoint Epoch_20.pt to disk...
INFO 2022-05-07 23:43:13 train.py: 88] Epoch 21, iter 0/628, lr 0.000314, loss 0.173587
INFO 2022-05-07 23:49:02 train.py: 88] Epoch 21, iter 200/628, lr 0.000309, loss 0.150622
INFO 2022-05-07 23:54:51 train.py: 88] Epoch 21, iter 400/628, lr 0.000304, loss 0.136907
INFO 2022-05-08 00:00:40 train.py: 88] Epoch 21, iter 600/628, lr 0.000300, loss 0.130032
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
INFO 2022-05-08 00:01:27 train.py: 108] Save checkpoint Epoch_21.pt to disk...
INFO 2022-05-08 00:01:29 train.py: 88] Epoch 22, iter 0/628, lr 0.000299, loss 0.122561
INFO 2022-05-08 00:07:18 train.py: 88] Epoch 22, iter 200/628, lr 0.000294, loss 0.123314
INFO 2022-05-08 00:13:06 train.py: 88] Epoch 22, iter 400/628, lr 0.000289, loss 0.110982
INFO 2022-05-08 00:18:54 train.py: 88] Epoch 22, iter 600/628, lr 0.000284, loss 0.109390
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
INFO 2022-05-08 00:19:41 train.py: 108] Save checkpoint Epoch_22.pt to disk...
INFO 2022-05-08 00:19:42 train.py: 88] Epoch 23, iter 0/628, lr 0.000284, loss 0.105509
INFO 2022-05-08 00:25:31 train.py: 88] Epoch 23, iter 200/628, lr 0.000279, loss 0.098436
INFO 2022-05-08 00:31:19 train.py: 88] Epoch 23, iter 400/628, lr 0.000274, loss 0.089901
INFO 2022-05-08 00:37:08 train.py: 88] Epoch 23, iter 600/628, lr 0.000269, loss 0.091080
INFO 2022-05-08 00:37:55 train.py: 108] Save checkpoint Epoch_23.pt to disk...
INFO 2022-05-08 00:37:56 train.py: 88] Epoch 24, iter 0/628, lr 0.000268, loss 0.077066
INFO 2022-05-08 00:43:45 train.py: 88] Epoch 24, iter 200/628, lr 0.000263, loss 0.080952
INFO 2022-05-08 00:49:33 train.py: 88] Epoch 24, iter 400/628, lr 0.000258, loss 0.082721
INFO 2022-05-08 00:55:22 train.py: 88] Epoch 24, iter 600/628, lr 0.000253, loss 0.072428
INFO 2022-05-08 00:56:08 train.py: 108] Save checkpoint Epoch_24.pt to disk...
INFO 2022-05-08 00:56:10 train.py: 88] Epoch 25, iter 0/628, lr 0.000253, loss 0.075479
INFO 2022-05-08 01:01:58 train.py: 88] Epoch 25, iter 200/628, lr 0.000248, loss 0.069330
INFO 2022-05-08 01:07:46 train.py: 88] Epoch 25, iter 400/628, lr 0.000243, loss 0.067124
INFO 2022-05-08 01:13:34 train.py: 88] Epoch 25, iter 600/628, lr 0.000238, loss 0.061832
INFO 2022-05-08 01:14:20 train.py: 108] Save checkpoint Epoch_25.pt to disk...
INFO 2022-05-08 01:14:22 train.py: 88] Epoch 26, iter 0/628, lr 0.000237, loss 0.062440
INFO 2022-05-08 01:20:10 train.py: 88] Epoch 26, iter 200/628, lr 0.000232, loss 0.060246
INFO 2022-05-08 01:25:57 train.py: 88] Epoch 26, iter 400/628, lr 0.000227, loss 0.051430
INFO 2022-05-08 01:31:43 train.py: 88] Epoch 26, iter 600/628, lr 0.000222, loss 0.047478
INFO 2022-05-08 01:32:30 train.py: 108] Save checkpoint Epoch_26.pt to disk...
INFO 2022-05-08 01:32:31 train.py: 88] Epoch 27, iter 0/628, lr 0.000221, loss 0.044845
INFO 2022-05-08 01:38:17 train.py: 88] Epoch 27, iter 200/628, lr 0.000217, loss 0.048644
INFO 2022-05-08 01:44:05 train.py: 88] Epoch 27, iter 400/628, lr 0.000212, loss 0.039710
INFO 2022-05-08 01:49:52 train.py: 88] Epoch 27, iter 600/628, lr 0.000207, loss 0.036682
INFO 2022-05-08 01:50:38 train.py: 108] Save checkpoint Epoch_27.pt to disk...
INFO 2022-05-08 01:50:40 train.py: 88] Epoch 28, iter 0/628, lr 0.000206, loss 0.033198
INFO 2022-05-08 01:56:27 train.py: 88] Epoch 28, iter 200/628, lr 0.000201, loss 0.032170
INFO 2022-05-08 02:02:13 train.py: 88] Epoch 28, iter 400/628, lr 0.000196, loss 0.032135
INFO 2022-05-08 02:08:00 train.py: 88] Epoch 28, iter 600/628, lr 0.000192, loss 0.028379
INFO 2022-05-08 02:08:46 train.py: 108] Save checkpoint Epoch_28.pt to disk...
INFO 2022-05-08 02:08:48 train.py: 88] Epoch 29, iter 0/628, lr 0.000191, loss 0.028989
INFO 2022-05-08 02:14:35 train.py: 88] Epoch 29, iter 200/628, lr 0.000186, loss 0.025853
INFO 2022-05-08 02:20:22 train.py: 88] Epoch 29, iter 400/628, lr 0.000181, loss 0.024926
INFO 2022-05-08 02:26:09 train.py: 88] Epoch 29, iter 600/628, lr 0.000177, loss 0.023874
INFO 2022-05-08 02:26:55 train.py: 108] Save checkpoint Epoch_29.pt to disk...
INFO 2022-05-08 02:26:57 train.py: 88] Epoch 30, iter 0/628, lr 0.000176, loss 0.020596
INFO 2022-05-08 02:32:43 train.py: 88] Epoch 30, iter 200/628, lr 0.000171, loss 0.022414
INFO 2022-05-08 02:38:29 train.py: 88] Epoch 30, iter 400/628, lr 0.000167, loss 0.022877
INFO 2022-05-08 02:44:15 train.py: 88] Epoch 30, iter 600/628, lr 0.000162, loss 0.021555
INFO 2022-05-08 02:45:01 train.py: 108] Save checkpoint Epoch_30.pt to disk...
INFO 2022-05-08 02:45:03 train.py: 88] Epoch 31, iter 0/628, lr 0.000161, loss 0.020229
INFO 2022-05-08 02:50:49 train.py: 88] Epoch 31, iter 200/628, lr 0.000157, loss 0.020688
INFO 2022-05-08 02:56:35 train.py: 88] Epoch 31, iter 400/628, lr 0.000152, loss 0.020923
INFO 2022-05-08 03:02:22 train.py: 88] Epoch 31, iter 600/628, lr 0.000148, loss 0.021450
INFO 2022-05-08 03:03:08 train.py: 108] Save checkpoint Epoch_31.pt to disk...
INFO 2022-05-08 03:03:09 train.py: 88] Epoch 32, iter 0/628, lr 0.000147, loss 0.025980
INFO 2022-05-08 03:08:57 train.py: 88] Epoch 32, iter 200/628, lr 0.000143, loss 0.022343
INFO 2022-05-08 03:14:44 train.py: 88] Epoch 32, iter 400/628, lr 0.000138, loss 0.025639
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
INFO 2022-05-08 03:20:31 train.py: 88] Epoch 32, iter 600/628, lr 0.000134, loss 0.023036
INFO 2022-05-08 03:21:18 train.py: 108] Save checkpoint Epoch_32.pt to disk...
INFO 2022-05-08 03:21:19 train.py: 88] Epoch 33, iter 0/628, lr 0.000133, loss 0.019370
INFO 2022-05-08 03:27:06 train.py: 88] Epoch 33, iter 200/628, lr 0.000129, loss 0.017672
INFO 2022-05-08 03:32:52 train.py: 88] Epoch 33, iter 400/628, lr 0.000125, loss 0.013880
INFO 2022-05-08 03:38:38 train.py: 88] Epoch 33, iter 600/628, lr 0.000120, loss 0.015309
INFO 2022-05-08 03:39:25 train.py: 108] Save checkpoint Epoch_33.pt to disk...
INFO 2022-05-08 03:39:26 train.py: 88] Epoch 34, iter 0/628, lr 0.000120, loss 0.017475
INFO 2022-05-08 03:45:12 train.py: 88] Epoch 34, iter 200/628, lr 0.000116, loss 0.016194
INFO 2022-05-08 03:50:58 train.py: 88] Epoch 34, iter 400/628, lr 0.000112, loss 0.015892
INFO 2022-05-08 03:56:45 train.py: 88] Epoch 34, iter 600/628, lr 0.000108, loss 0.013340
INFO 2022-05-08 03:57:32 train.py: 108] Save checkpoint Epoch_34.pt to disk...
INFO 2022-05-08 03:57:34 train.py: 88] Epoch 35, iter 0/628, lr 0.000107, loss 0.010662
INFO 2022-05-08 04:03:21 train.py: 88] Epoch 35, iter 200/628, lr 0.000103, loss 0.011641
INFO 2022-05-08 04:09:07 train.py: 88] Epoch 35, iter 400/628, lr 0.000099, loss 0.010684
INFO 2022-05-08 04:14:54 train.py: 88] Epoch 35, iter 600/628, lr 0.000095, loss 0.009989
INFO 2022-05-08 04:15:41 train.py: 108] Save checkpoint Epoch_35.pt to disk...
INFO 2022-05-08 04:15:42 train.py: 88] Epoch 36, iter 0/628, lr 0.000095, loss 0.008623
INFO 2022-05-08 04:21:29 train.py: 88] Epoch 36, iter 200/628, lr 0.000091, loss 0.009060
INFO 2022-05-08 04:27:15 train.py: 88] Epoch 36, iter 400/628, lr 0.000087, loss 0.009009
INFO 2022-05-08 04:33:03 train.py: 88] Epoch 36, iter 600/628, lr 0.000084, loss 0.008698
INFO 2022-05-08 04:33:50 train.py: 108] Save checkpoint Epoch_36.pt to disk...
INFO 2022-05-08 04:33:51 train.py: 88] Epoch 37, iter 0/628, lr 0.000083, loss 0.007842
INFO 2022-05-08 04:39:38 train.py: 88] Epoch 37, iter 200/628, lr 0.000079, loss 0.007628
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
INFO 2022-05-08 04:45:24 train.py: 88] Epoch 37, iter 400/628, lr 0.000076, loss 0.008267
INFO 2022-05-08 04:51:11 train.py: 88] Epoch 37, iter 600/628, lr 0.000073, loss 0.007594
INFO 2022-05-08 04:51:57 train.py: 108] Save checkpoint Epoch_37.pt to disk...
INFO 2022-05-08 04:51:59 train.py: 88] Epoch 38, iter 0/628, lr 0.000072, loss 0.007522
INFO 2022-05-08 04:57:46 train.py: 88] Epoch 38, iter 200/628, lr 0.000069, loss 0.007083
INFO 2022-05-08 05:03:32 train.py: 88] Epoch 38, iter 400/628, lr 0.000065, loss 0.006774
INFO 2022-05-08 05:09:17 train.py: 88] Epoch 38, iter 600/628, lr 0.000062, loss 0.006421
INFO 2022-05-08 05:10:03 train.py: 108] Save checkpoint Epoch_38.pt to disk...
INFO 2022-05-08 05:10:05 train.py: 88] Epoch 39, iter 0/628, lr 0.000062, loss 0.006366
INFO 2022-05-08 05:15:52 train.py: 88] Epoch 39, iter 200/628, lr 0.000059, loss 0.006034
INFO 2022-05-08 05:21:39 train.py: 88] Epoch 39, iter 400/628, lr 0.000056, loss 0.006078
INFO 2022-05-08 05:27:25 train.py: 88] Epoch 39, iter 600/628, lr 0.000053, loss 0.006106
INFO 2022-05-08 05:28:11 train.py: 108] Save checkpoint Epoch_39.pt to disk...
INFO 2022-05-08 05:28:13 train.py: 88] Epoch 40, iter 0/628, lr 0.000052, loss 0.006013
INFO 2022-05-08 05:34:00 train.py: 88] Epoch 40, iter 200/628, lr 0.000049, loss 0.005783
INFO 2022-05-08 05:39:47 train.py: 88] Epoch 40, iter 400/628, lr 0.000047, loss 0.005333
INFO 2022-05-08 05:45:33 train.py: 88] Epoch 40, iter 600/628, lr 0.000044, loss 0.005194
INFO 2022-05-08 05:46:20 train.py: 108] Save checkpoint Epoch_40.pt to disk...
INFO 2022-05-08 05:46:21 train.py: 88] Epoch 41, iter 0/628, lr 0.000044, loss 0.005151
INFO 2022-05-08 05:52:07 train.py: 88] Epoch 41, iter 200/628, lr 0.000041, loss 0.004971
INFO 2022-05-08 05:57:53 train.py: 88] Epoch 41, iter 400/628, lr 0.000038, loss 0.004914
INFO 2022-05-08 06:03:39 train.py: 88] Epoch 41, iter 600/628, lr 0.000036, loss 0.004607
INFO 2022-05-08 06:04:26 train.py: 108] Save checkpoint Epoch_41.pt to disk...
INFO 2022-05-08 06:04:27 train.py: 88] Epoch 42, iter 0/628, lr 0.000036, loss 0.004608
INFO 2022-05-08 06:10:14 train.py: 88] Epoch 42, iter 200/628, lr 0.000033, loss 0.004657
INFO 2022-05-08 06:16:00 train.py: 88] Epoch 42, iter 400/628, lr 0.000031, loss 0.004523
INFO 2022-05-08 06:21:47 train.py: 88] Epoch 42, iter 600/628, lr 0.000029, loss 0.004449
INFO 2022-05-08 06:22:34 train.py: 108] Save checkpoint Epoch_42.pt to disk...
INFO 2022-05-08 06:22:35 train.py: 88] Epoch 43, iter 0/628, lr 0.000029, loss 0.004124
INFO 2022-05-08 06:28:22 train.py: 88] Epoch 43, iter 200/628, lr 0.000026, loss 0.004398
INFO 2022-05-08 06:34:08 train.py: 88] Epoch 43, iter 400/628, lr 0.000025, loss 0.004223
INFO 2022-05-08 06:39:54 train.py: 88] Epoch 43, iter 600/628, lr 0.000023, loss 0.003977
INFO 2022-05-08 06:40:41 train.py: 108] Save checkpoint Epoch_43.pt to disk...
INFO 2022-05-08 06:40:42 train.py: 88] Epoch 44, iter 0/628, lr 0.000022, loss 0.004060
INFO 2022-05-08 06:46:28 train.py: 88] Epoch 44, iter 200/628, lr 0.000021, loss 0.004013
INFO 2022-05-08 06:52:14 train.py: 88] Epoch 44, iter 400/628, lr 0.000019, loss 0.003964
INFO 2022-05-08 06:58:01 train.py: 88] Epoch 44, iter 600/628, lr 0.000017, loss 0.003796
INFO 2022-05-08 06:58:47 train.py: 108] Save checkpoint Epoch_44.pt to disk...
INFO 2022-05-08 06:58:48 train.py: 88] Epoch 45, iter 0/628, lr 0.000017, loss 0.003704
INFO 2022-05-08 07:04:35 train.py: 88] Epoch 45, iter 200/628, lr 0.000016, loss 0.003772
INFO 2022-05-08 07:10:21 train.py: 88] Epoch 45, iter 400/628, lr 0.000014, loss 0.003668
INFO 2022-05-08 07:16:08 train.py: 88] Epoch 45, iter 600/628, lr 0.000013, loss 0.003708
Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.8/site-packages/torch/serialization.py", line 372, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/serialization.py", line 491, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
OSError: [Errno 28] No space left on device

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "train.py", line 208, in <module>
    train(args)
  File "train.py", line 157, in train
    train_one_epoch(train_loader, model, optimizer, lr_schedule,
  File "train.py", line 107, in train_one_epoch
    torch.save(state, os.path.join(args.out_dir, saved_name))
  File "/root/miniconda3/lib/python3.8/site-packages/torch/serialization.py", line 373, in save
    return
  File "/root/miniconda3/lib/python3.8/site-packages/torch/serialization.py", line 259, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:274] . unexpected pos 3898240 vs 3898128
terminate called after throwing an instance of 'c10::Error'
  what():  [enforce fail at inline_container.cc:274] . unexpected pos 3898240 vs 3898128
frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x7f37c68b30e7 in /root/miniconda3/lib/python3.8/site-packages/torch/lib/libc10.so)
frame #1: <unknown function> + 0x207e700 (0x7f37c8da4700 in /root/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #2: <unknown function> + 0x207a8d3 (0x7f37c8da08d3 in /root/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xa9 (0x7f37c8da5079 in /root/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0xe1 (0x7f37c8da5bb1 in /root/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x115 (0x7f37c8da63a5 in /root/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch_cpu.so)
frame #6: <unknown function> + 0x737cf3 (0x7f383a715cf3 in /root/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #7: <unknown function> + 0x369f80 (0x7f383a347f80 in /root/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #8: <unknown function> + 0x36b1ee (0x7f383a3491ee in /root/miniconda3/lib/python3.8/site-packages/torch/lib/libtorch_python.so)
frame #9: <unknown function> + 0x1592ac (0x559b120002ac in /root/miniconda3/bin/python)
frame #10: <unknown function> + 0x158e77 (0x559b11fffe77 in /root/miniconda3/bin/python)
frame #11: <unknown function> + 0x158e60 (0x559b11fffe60 in /root/miniconda3/bin/python)
frame #12: <unknown function> + 0x158e60 (0x559b11fffe60 in /root/miniconda3/bin/python)
frame #13: <unknown function> + 0x158e60 (0x559b11fffe60 in /root/miniconda3/bin/python)
frame #14: <unknown function> + 0x176057 (0x559b1201d057 in /root/miniconda3/bin/python)
frame #15: PyDict_SetItemString + 0x61 (0x559b1203e3c1 in /root/miniconda3/bin/python)
frame #16: PyImport_Cleanup + 0x9d (0x559b1207caad in /root/miniconda3/bin/python)
frame #17: Py_FinalizeEx + 0x79 (0x559b120aea49 in /root/miniconda3/bin/python)
frame #18: Py_RunMain + 0x183 (0x559b120b0893 in /root/miniconda3/bin/python)
frame #19: Py_BytesMain + 0x39 (0x559b120b0ca9 in /root/miniconda3/bin/python)
frame #20: __libc_start_main + 0xe7 (0x7f383cedabf7 in /lib/x86_64-linux-gnu/libc.so.6)
frame #21: <unknown function> + 0x1e21c7 (0x559b120891c7 in /root/miniconda3/bin/python)

Traceback (most recent call last):
  File "/root/miniconda3/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/miniconda3/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 340, in <module>
    main()
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 326, in main
    sigkill_handler(signal.SIGTERM, None)  # not coming back
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributed/launch.py", line 301, in sigkill_handler
    raise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)
subprocess.CalledProcessError: Command '['/root/miniconda3/bin/python', '-u', 'train.py', '--local_rank=1', '--data_root', 'train_data/msra_tiny', '--train_file', 'train_data/msra_tiny_train_file.txt', '--backbone_type', 'SwinTransformer', '--backbone_conf_file', '../backbone_conf.yaml', '--head_type', 'MV-Softmax', '--head_conf_file', '../head_conf.yaml', '--lr', '5e-4', '--out_dir', 'out_dir', '--epoches', '50', '--warm_up_epoches', '10', '--print_freq', '200', '--save_freq', '3000', '--batch_size', '64', '--log_dir', 'log', '--tensorboardx_logdir', 'mv-swin']' died with <Signals.SIGABRT: 6>.
Killing subprocess 40570
Killing subprocess 40571
