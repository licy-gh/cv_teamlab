/root/miniconda3/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
/root/miniconda3/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
Use GPU: 1 for training
Use GPU: 0 for training
INFO 2022-05-12 02:24:46 distributed_c10d.py: 187] Added key: store_based_barrier_key:1 to store for rank: 1
INFO 2022-05-12 02:24:46 distributed_c10d.py: 187] Added key: store_based_barrier_key:1 to store for rank: 0
backbone param:
{'img_size': 224, 'patch_size': 4, 'in_chans': 3, 'embed_dim': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'drop_rate': 0.0, 'drop_path_rate': 0.3}
backbone param:
{'img_size': 224, 'patch_size': 4, 'in_chans': 3, 'embed_dim': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'drop_rate': 0.0, 'drop_path_rate': 0.3}
head param:
{'feat_dim': 512, 'num_class': 72778, 'is_am': 1, 'margin': 0.35, 'mv_weight': 1.12, 'scale': 32}
head param:
{'feat_dim': 512, 'num_class': 72778, 'is_am': 1, 'margin': 0.35, 'mv_weight': 1.12, 'scale': 32}
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
INFO 2022-05-12 02:24:53 train.py: 88] Epoch 0, iter 0/628, lr 0.000000, loss 15.952081
INFO 2022-05-12 02:24:53 distributed.py: 693] Reducer buckets have been rebuilt in this iteration.
INFO 2022-05-12 02:24:53 distributed.py: 693] Reducer buckets have been rebuilt in this iteration.
INFO 2022-05-12 02:28:50 train.py: 88] Epoch 0, iter 200/628, lr 0.000054, loss 15.653065
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
INFO 2022-05-12 02:32:46 train.py: 88] Epoch 0, iter 400/628, lr 0.000107, loss 13.303278
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
INFO 2022-05-12 02:36:47 train.py: 88] Epoch 0, iter 600/628, lr 0.000160, loss 12.103088
INFO 2022-05-12 02:37:25 train.py: 108] Save checkpoint Epoch_0.pt to disk...
INFO 2022-05-12 02:37:26 train.py: 88] Epoch 1, iter 0/628, lr 0.000167, loss 11.843139
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
INFO 2022-05-12 02:41:22 train.py: 88] Epoch 1, iter 200/628, lr 0.000220, loss 11.917128
INFO 2022-05-12 02:45:15 train.py: 88] Epoch 1, iter 400/628, lr 0.000273, loss 11.839770
INFO 2022-05-12 02:49:18 train.py: 88] Epoch 1, iter 600/628, lr 0.000326, loss 11.532097
INFO 2022-05-12 02:49:50 train.py: 108] Save checkpoint Epoch_1.pt to disk...
INFO 2022-05-12 02:49:51 train.py: 88] Epoch 2, iter 0/628, lr 0.000334, loss 11.120319
INFO 2022-05-12 02:53:45 train.py: 88] Epoch 2, iter 200/628, lr 0.000387, loss 11.113531
INFO 2022-05-12 02:57:55 train.py: 88] Epoch 2, iter 400/628, lr 0.000440, loss 10.268465
INFO 2022-05-12 03:01:49 train.py: 88] Epoch 2, iter 600/628, lr 0.000493, loss 9.372267
INFO 2022-05-12 03:02:20 train.py: 108] Save checkpoint Epoch_2.pt to disk...
INFO 2022-05-12 03:02:21 train.py: 88] Epoch 3, iter 0/628, lr 0.000467, loss 8.974272
INFO 2022-05-12 03:06:26 train.py: 88] Epoch 3, iter 200/628, lr 0.000460, loss 8.704393
INFO 2022-05-12 03:10:17 train.py: 88] Epoch 3, iter 400/628, lr 0.000452, loss 8.307124
INFO 2022-05-12 03:14:09 train.py: 88] Epoch 3, iter 600/628, lr 0.000443, loss 8.110559
INFO 2022-05-12 03:14:42 train.py: 108] Save checkpoint Epoch_3.pt to disk...
INFO 2022-05-12 03:14:43 train.py: 88] Epoch 4, iter 0/628, lr 0.000442, loss 8.164841
INFO 2022-05-12 03:18:37 train.py: 88] Epoch 4, iter 200/628, lr 0.000433, loss 8.060077
INFO 2022-05-12 03:22:24 train.py: 88] Epoch 4, iter 400/628, lr 0.000423, loss 8.072201
INFO 2022-05-12 03:26:25 train.py: 88] Epoch 4, iter 600/628, lr 0.000413, loss 8.137872
INFO 2022-05-12 03:26:56 train.py: 108] Save checkpoint Epoch_4.pt to disk...
INFO 2022-05-12 03:26:57 train.py: 88] Epoch 5, iter 0/628, lr 0.000412, loss 8.070470
INFO 2022-05-12 03:30:44 train.py: 88] Epoch 5, iter 200/628, lr 0.000401, loss 8.265409
INFO 2022-05-12 03:34:44 train.py: 88] Epoch 5, iter 400/628, lr 0.000390, loss 8.353553
INFO 2022-05-12 03:38:31 train.py: 88] Epoch 5, iter 600/628, lr 0.000378, loss 8.404263
INFO 2022-05-12 03:39:02 train.py: 108] Save checkpoint Epoch_5.pt to disk...
INFO 2022-05-12 03:39:03 train.py: 88] Epoch 6, iter 0/628, lr 0.000376, loss 8.250617
INFO 2022-05-12 03:43:01 train.py: 88] Epoch 6, iter 200/628, lr 0.000364, loss 8.405149
INFO 2022-05-12 03:46:52 train.py: 88] Epoch 6, iter 400/628, lr 0.000352, loss 8.404073
INFO 2022-05-12 03:50:38 train.py: 88] Epoch 6, iter 600/628, lr 0.000339, loss 8.414237
INFO 2022-05-12 03:51:09 train.py: 108] Save checkpoint Epoch_6.pt to disk...
INFO 2022-05-12 03:51:10 train.py: 88] Epoch 7, iter 0/628, lr 0.000337, loss 8.195016
INFO 2022-05-12 03:54:57 train.py: 88] Epoch 7, iter 200/628, lr 0.000324, loss 8.333409
INFO 2022-05-12 03:58:53 train.py: 88] Epoch 7, iter 400/628, lr 0.000311, loss 8.219981
INFO 2022-05-12 04:02:40 train.py: 88] Epoch 7, iter 600/628, lr 0.000297, loss 8.029652
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
INFO 2022-05-12 04:03:10 train.py: 108] Save checkpoint Epoch_7.pt to disk...
INFO 2022-05-12 04:03:11 train.py: 88] Epoch 8, iter 0/628, lr 0.000295, loss 7.931922
INFO 2022-05-12 04:07:04 train.py: 88] Epoch 8, iter 200/628, lr 0.000282, loss 7.851734
INFO 2022-05-12 04:10:53 train.py: 88] Epoch 8, iter 400/628, lr 0.000268, loss 7.634154
INFO 2022-05-12 04:14:39 train.py: 88] Epoch 8, iter 600/628, lr 0.000254, loss 7.362140
INFO 2022-05-12 04:15:12 train.py: 108] Save checkpoint Epoch_8.pt to disk...
INFO 2022-05-12 04:15:13 train.py: 88] Epoch 9, iter 0/628, lr 0.000253, loss 7.288212
INFO 2022-05-12 04:19:09 train.py: 88] Epoch 9, iter 200/628, lr 0.000239, loss 7.039235
INFO 2022-05-12 04:22:55 train.py: 88] Epoch 9, iter 400/628, lr 0.000225, loss 6.792090
INFO 2022-05-12 04:26:49 train.py: 88] Epoch 9, iter 600/628, lr 0.000211, loss 6.378547
INFO 2022-05-12 04:27:20 train.py: 108] Save checkpoint Epoch_9.pt to disk...
INFO 2022-05-12 04:27:21 train.py: 88] Epoch 10, iter 0/628, lr 0.000210, loss 6.360911
INFO 2022-05-12 04:31:08 train.py: 88] Epoch 10, iter 200/628, lr 0.000196, loss 6.054423
INFO 2022-05-12 04:35:00 train.py: 88] Epoch 10, iter 400/628, lr 0.000183, loss 5.744921
INFO 2022-05-12 04:38:53 train.py: 88] Epoch 10, iter 600/628, lr 0.000170, loss 5.348976
INFO 2022-05-12 04:39:24 train.py: 108] Save checkpoint Epoch_10.pt to disk...
INFO 2022-05-12 04:39:25 train.py: 88] Epoch 11, iter 0/628, lr 0.000168, loss 5.210507
INFO 2022-05-12 04:43:11 train.py: 88] Epoch 11, iter 200/628, lr 0.000155, loss 5.018684
INFO 2022-05-12 04:47:07 train.py: 88] Epoch 11, iter 400/628, lr 0.000143, loss 4.656049
INFO 2022-05-12 04:50:55 train.py: 88] Epoch 11, iter 600/628, lr 0.000130, loss 4.314573
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
INFO 2022-05-12 04:51:25 train.py: 108] Save checkpoint Epoch_11.pt to disk...
INFO 2022-05-12 04:51:26 train.py: 88] Epoch 12, iter 0/628, lr 0.000129, loss 4.198216
INFO 2022-05-12 04:55:21 train.py: 88] Epoch 12, iter 200/628, lr 0.000117, loss 4.033964
INFO 2022-05-12 04:59:08 train.py: 88] Epoch 12, iter 400/628, lr 0.000106, loss 3.720509
INFO 2022-05-12 05:02:57 train.py: 88] Epoch 12, iter 600/628, lr 0.000095, loss 3.407521
INFO 2022-05-12 05:03:30 train.py: 108] Save checkpoint Epoch_12.pt to disk...
INFO 2022-05-12 05:03:31 train.py: 88] Epoch 13, iter 0/628, lr 0.000093, loss 3.309107
INFO 2022-05-12 05:07:23 train.py: 88] Epoch 13, iter 200/628, lr 0.000083, loss 3.144309
INFO 2022-05-12 05:11:10 train.py: 88] Epoch 13, iter 400/628, lr 0.000073, loss 2.928676
INFO 2022-05-12 05:14:58 train.py: 88] Epoch 13, iter 600/628, lr 0.000064, loss 2.708918
INFO 2022-05-12 05:15:29 train.py: 108] Save checkpoint Epoch_13.pt to disk...
INFO 2022-05-12 05:15:30 train.py: 88] Epoch 14, iter 0/628, lr 0.000063, loss 2.703996
INFO 2022-05-12 05:19:31 train.py: 88] Epoch 14, iter 200/628, lr 0.000054, loss 2.513853
INFO 2022-05-12 05:23:19 train.py: 88] Epoch 14, iter 400/628, lr 0.000046, loss 2.366817
INFO 2022-05-12 05:27:13 train.py: 88] Epoch 14, iter 600/628, lr 0.000039, loss 2.182567
INFO 2022-05-12 05:27:46 train.py: 108] Save checkpoint Epoch_14.pt to disk...
INFO 2022-05-12 05:27:47 train.py: 88] Epoch 15, iter 0/628, lr 0.000038, loss 2.171584
INFO 2022-05-12 05:31:35 train.py: 88] Epoch 15, iter 200/628, lr 0.000032, loss 2.091029
INFO 2022-05-12 05:35:21 train.py: 88] Epoch 15, iter 400/628, lr 0.000026, loss 1.990424
INFO 2022-05-12 05:39:20 train.py: 88] Epoch 15, iter 600/628, lr 0.000021, loss 1.891356
INFO 2022-05-12 05:39:51 train.py: 108] Save checkpoint Epoch_15.pt to disk...
INFO 2022-05-12 05:39:52 train.py: 88] Epoch 16, iter 0/628, lr 0.000020, loss 1.887194
INFO 2022-05-12 05:43:41 train.py: 88] Epoch 16, iter 200/628, lr 0.000016, loss 1.868922
INFO 2022-05-12 05:47:38 train.py: 88] Epoch 16, iter 400/628, lr 0.000012, loss 1.804079
INFO 2022-05-12 05:51:25 train.py: 88] Epoch 16, iter 600/628, lr 0.000009, loss 1.690216
INFO 2022-05-12 05:51:55 train.py: 108] Save checkpoint Epoch_16.pt to disk...
INFO 2022-05-12 05:51:56 train.py: 88] Epoch 17, iter 0/628, lr 0.000009, loss 1.808692
INFO 2022-05-12 05:55:49 train.py: 88] Epoch 17, iter 200/628, lr 0.000007, loss 1.727645
INFO 2022-05-12 05:59:38 train.py: 88] Epoch 17, iter 400/628, lr 0.000005, loss 1.720107
INFO 2022-05-12 06:03:25 train.py: 88] Epoch 17, iter 600/628, lr 0.000005, loss 1.641144
INFO 2022-05-12 06:03:55 train.py: 108] Save checkpoint Epoch_17.pt to disk...
