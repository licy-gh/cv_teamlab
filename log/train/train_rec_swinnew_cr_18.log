/root/miniconda3/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
/root/miniconda3/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
Use GPU: 0 for training
Use GPU: 1 for training
INFO 2022-05-12 09:43:23 distributed_c10d.py: 187] Added key: store_based_barrier_key:1 to store for rank: 1
INFO 2022-05-12 09:43:23 distributed_c10d.py: 187] Added key: store_based_barrier_key:1 to store for rank: 0
backbone param:
{'img_size': 224, 'patch_size': 4, 'in_chans': 3, 'embed_dim': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'drop_rate': 0.0, 'drop_path_rate': 0.3}
backbone param:
{'img_size': 224, 'patch_size': 4, 'in_chans': 3, 'embed_dim': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'drop_rate': 0.0, 'drop_path_rate': 0.3}
head param:
{'feat_dim': 512, 'num_class': 72778, 'is_am': 1, 'margin': 0.35, 'mv_weight': 1.12, 'scale': 32}
head param:
{'feat_dim': 512, 'num_class': 72778, 'is_am': 1, 'margin': 0.35, 'mv_weight': 1.12, 'scale': 32}
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
INFO 2022-05-12 09:43:31 train.py: 88] Epoch 0, iter 0/628, lr 0.000000, loss 15.994329
INFO 2022-05-12 09:43:31 distributed.py: 693] Reducer buckets have been rebuilt in this iteration.
INFO 2022-05-12 09:43:31 distributed.py: 693] Reducer buckets have been rebuilt in this iteration.
INFO 2022-05-12 09:47:32 train.py: 88] Epoch 0, iter 200/628, lr 0.000054, loss 15.630615
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
INFO 2022-05-12 09:51:35 train.py: 88] Epoch 0, iter 400/628, lr 0.000107, loss 13.761485
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
INFO 2022-05-12 09:55:35 train.py: 88] Epoch 0, iter 600/628, lr 0.000160, loss 12.171486
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
INFO 2022-05-12 09:56:07 train.py: 108] Save checkpoint Epoch_0.pt to disk...
INFO 2022-05-12 09:56:09 train.py: 88] Epoch 1, iter 0/628, lr 0.000167, loss 12.025463
INFO 2022-05-12 10:00:13 train.py: 88] Epoch 1, iter 200/628, lr 0.000220, loss 12.104474
INFO 2022-05-12 10:04:13 train.py: 88] Epoch 1, iter 400/628, lr 0.000273, loss 11.798554
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
INFO 2022-05-12 10:08:17 train.py: 88] Epoch 1, iter 600/628, lr 0.000326, loss 11.428644
INFO 2022-05-12 10:08:50 train.py: 108] Save checkpoint Epoch_1.pt to disk...
INFO 2022-05-12 10:08:51 train.py: 88] Epoch 2, iter 0/628, lr 0.000334, loss 11.464854
INFO 2022-05-12 10:12:52 train.py: 88] Epoch 2, iter 200/628, lr 0.000387, loss 11.144659
INFO 2022-05-12 10:16:55 train.py: 88] Epoch 2, iter 400/628, lr 0.000440, loss 10.710098
INFO 2022-05-12 10:20:55 train.py: 88] Epoch 2, iter 600/628, lr 0.000493, loss 10.268835
INFO 2022-05-12 10:21:27 train.py: 108] Save checkpoint Epoch_2.pt to disk...
INFO 2022-05-12 10:21:28 train.py: 88] Epoch 3, iter 0/628, lr 0.000467, loss 10.053300
INFO 2022-05-12 10:25:27 train.py: 88] Epoch 3, iter 200/628, lr 0.000460, loss 9.861702
INFO 2022-05-12 10:29:29 train.py: 88] Epoch 3, iter 400/628, lr 0.000452, loss 9.506662
INFO 2022-05-12 10:33:27 train.py: 88] Epoch 3, iter 600/628, lr 0.000443, loss 9.135668
INFO 2022-05-12 10:33:59 train.py: 108] Save checkpoint Epoch_3.pt to disk...
INFO 2022-05-12 10:34:00 train.py: 88] Epoch 4, iter 0/628, lr 0.000442, loss 9.147438
INFO 2022-05-12 10:37:57 train.py: 88] Epoch 4, iter 200/628, lr 0.000433, loss 9.042357
INFO 2022-05-12 10:41:58 train.py: 88] Epoch 4, iter 400/628, lr 0.000423, loss 8.948209
INFO 2022-05-12 10:45:55 train.py: 88] Epoch 4, iter 600/628, lr 0.000413, loss 8.735148
INFO 2022-05-12 10:46:27 train.py: 108] Save checkpoint Epoch_4.pt to disk...
INFO 2022-05-12 10:46:28 train.py: 88] Epoch 5, iter 0/628, lr 0.000412, loss 8.748102
INFO 2022-05-12 10:50:29 train.py: 88] Epoch 5, iter 200/628, lr 0.000401, loss 8.514123
INFO 2022-05-12 10:54:27 train.py: 88] Epoch 5, iter 400/628, lr 0.000390, loss 8.276553
INFO 2022-05-12 10:58:24 train.py: 88] Epoch 5, iter 600/628, lr 0.000378, loss 8.033562
INFO 2022-05-12 10:58:57 train.py: 108] Save checkpoint Epoch_5.pt to disk...
INFO 2022-05-12 10:58:58 train.py: 88] Epoch 6, iter 0/628, lr 0.000376, loss 8.006035
INFO 2022-05-12 11:02:57 train.py: 88] Epoch 6, iter 200/628, lr 0.000364, loss 7.822021
INFO 2022-05-12 11:06:53 train.py: 88] Epoch 6, iter 400/628, lr 0.000352, loss 7.642409
INFO 2022-05-12 11:10:53 train.py: 88] Epoch 6, iter 600/628, lr 0.000339, loss 7.446479
INFO 2022-05-12 11:11:25 train.py: 108] Save checkpoint Epoch_6.pt to disk...
INFO 2022-05-12 11:11:26 train.py: 88] Epoch 7, iter 0/628, lr 0.000337, loss 7.512135
INFO 2022-05-12 11:15:22 train.py: 88] Epoch 7, iter 200/628, lr 0.000324, loss 7.236453
INFO 2022-05-12 11:19:21 train.py: 88] Epoch 7, iter 400/628, lr 0.000311, loss 7.064920
INFO 2022-05-12 11:23:17 train.py: 88] Epoch 7, iter 600/628, lr 0.000297, loss 6.846420
INFO 2022-05-12 11:23:48 train.py: 108] Save checkpoint Epoch_7.pt to disk...
INFO 2022-05-12 11:23:49 train.py: 88] Epoch 8, iter 0/628, lr 0.000295, loss 6.729074
INFO 2022-05-12 11:27:47 train.py: 88] Epoch 8, iter 200/628, lr 0.000282, loss 6.753252
INFO 2022-05-12 11:31:45 train.py: 88] Epoch 8, iter 400/628, lr 0.000268, loss 6.517188
INFO 2022-05-12 11:35:41 train.py: 88] Epoch 8, iter 600/628, lr 0.000254, loss 6.311525
INFO 2022-05-12 11:36:13 train.py: 108] Save checkpoint Epoch_8.pt to disk...
INFO 2022-05-12 11:36:14 train.py: 88] Epoch 9, iter 0/628, lr 0.000253, loss 6.246067
INFO 2022-05-12 11:40:14 train.py: 88] Epoch 9, iter 200/628, lr 0.000239, loss 6.214651
INFO 2022-05-12 11:44:08 train.py: 88] Epoch 9, iter 400/628, lr 0.000225, loss 6.035390
INFO 2022-05-12 11:48:06 train.py: 88] Epoch 9, iter 600/628, lr 0.000211, loss 6.490265
INFO 2022-05-12 11:48:38 train.py: 108] Save checkpoint Epoch_9.pt to disk...
INFO 2022-05-12 11:48:39 train.py: 88] Epoch 10, iter 0/628, lr 0.000210, loss 6.542521
INFO 2022-05-12 11:52:33 train.py: 88] Epoch 10, iter 200/628, lr 0.000196, loss 6.338343
INFO 2022-05-12 11:56:27 train.py: 88] Epoch 10, iter 400/628, lr 0.000183, loss 6.061079
INFO 2022-05-12 12:00:22 train.py: 88] Epoch 10, iter 600/628, lr 0.000170, loss 5.846409
INFO 2022-05-12 12:00:55 train.py: 108] Save checkpoint Epoch_10.pt to disk...
INFO 2022-05-12 12:00:56 train.py: 88] Epoch 11, iter 0/628, lr 0.000168, loss 5.820424
INFO 2022-05-12 12:04:52 train.py: 88] Epoch 11, iter 200/628, lr 0.000155, loss 5.751528
INFO 2022-05-12 12:08:45 train.py: 88] Epoch 11, iter 400/628, lr 0.000143, loss 5.579035
INFO 2022-05-12 12:12:44 train.py: 88] Epoch 11, iter 600/628, lr 0.000130, loss 5.437896
INFO 2022-05-12 12:13:16 train.py: 108] Save checkpoint Epoch_11.pt to disk...
INFO 2022-05-12 12:13:17 train.py: 88] Epoch 12, iter 0/628, lr 0.000129, loss 5.457977
INFO 2022-05-12 12:17:12 train.py: 88] Epoch 12, iter 200/628, lr 0.000117, loss 5.369533
INFO 2022-05-12 12:21:10 train.py: 88] Epoch 12, iter 400/628, lr 0.000106, loss 5.236267
INFO 2022-05-12 12:25:05 train.py: 88] Epoch 12, iter 600/628, lr 0.000095, loss 5.138959
INFO 2022-05-12 12:25:36 train.py: 108] Save checkpoint Epoch_12.pt to disk...
INFO 2022-05-12 12:25:37 train.py: 88] Epoch 13, iter 0/628, lr 0.000093, loss 5.206399
INFO 2022-05-12 12:29:35 train.py: 88] Epoch 13, iter 200/628, lr 0.000083, loss 5.154512
INFO 2022-05-12 12:33:30 train.py: 88] Epoch 13, iter 400/628, lr 0.000073, loss 5.070302
INFO 2022-05-12 12:37:24 train.py: 88] Epoch 13, iter 600/628, lr 0.000064, loss 4.994799
INFO 2022-05-12 12:37:57 train.py: 108] Save checkpoint Epoch_13.pt to disk...
INFO 2022-05-12 12:37:58 train.py: 88] Epoch 14, iter 0/628, lr 0.000063, loss 5.060073
INFO 2022-05-12 12:41:55 train.py: 88] Epoch 14, iter 200/628, lr 0.000054, loss 5.009569
INFO 2022-05-12 12:45:48 train.py: 88] Epoch 14, iter 400/628, lr 0.000046, loss 4.952321
INFO 2022-05-12 12:49:45 train.py: 88] Epoch 14, iter 600/628, lr 0.000039, loss 4.905064
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
INFO 2022-05-12 12:50:17 train.py: 108] Save checkpoint Epoch_14.pt to disk...
INFO 2022-05-12 12:50:18 train.py: 88] Epoch 15, iter 0/628, lr 0.000038, loss 4.908438
INFO 2022-05-12 12:54:11 train.py: 88] Epoch 15, iter 200/628, lr 0.000032, loss 4.907290
INFO 2022-05-12 12:58:07 train.py: 88] Epoch 15, iter 400/628, lr 0.000026, loss 4.885180
INFO 2022-05-12 13:02:01 train.py: 88] Epoch 15, iter 600/628, lr 0.000021, loss 4.844534
INFO 2022-05-12 13:02:32 train.py: 108] Save checkpoint Epoch_15.pt to disk...
INFO 2022-05-12 13:02:33 train.py: 88] Epoch 16, iter 0/628, lr 0.000020, loss 4.936906
INFO 2022-05-12 13:06:28 train.py: 88] Epoch 16, iter 200/628, lr 0.000016, loss 4.860378
INFO 2022-05-12 13:10:25 train.py: 88] Epoch 16, iter 400/628, lr 0.000012, loss 4.873520
INFO 2022-05-12 13:14:18 train.py: 88] Epoch 16, iter 600/628, lr 0.000009, loss 4.795940
INFO 2022-05-12 13:14:49 train.py: 108] Save checkpoint Epoch_16.pt to disk...
INFO 2022-05-12 13:14:50 train.py: 88] Epoch 17, iter 0/628, lr 0.000009, loss 4.907338
INFO 2022-05-12 13:18:43 train.py: 88] Epoch 17, iter 200/628, lr 0.000007, loss 4.845222
INFO 2022-05-12 13:22:40 train.py: 88] Epoch 17, iter 400/628, lr 0.000005, loss 4.812421
INFO 2022-05-12 13:26:34 train.py: 88] Epoch 17, iter 600/628, lr 0.000005, loss 4.791883
INFO 2022-05-12 13:27:05 train.py: 108] Save checkpoint Epoch_17.pt to disk...
