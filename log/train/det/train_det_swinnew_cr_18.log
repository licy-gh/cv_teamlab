/root/miniconda3/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
/root/miniconda3/lib/python3.8/site-packages/apex/pyprof/__init__.py:5: FutureWarning: pyprof will be removed by the end of June, 2022
  warnings.warn("pyprof will be removed by the end of June, 2022", FutureWarning)
Use GPU: 1 for training
Use GPU: 0 for training
INFO 2022-05-14 05:32:11 distributed_c10d.py: 187] Added key: store_based_barrier_key:1 to store for rank: 1
INFO 2022-05-14 05:32:11 distributed_c10d.py: 187] Added key: store_based_barrier_key:1 to store for rank: 0
backbone param:
{'img_size': 224, 'patch_size': 4, 'in_chans': 3, 'embed_dim': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'drop_rate': 0.0, 'drop_path_rate': 0.3}
backbone param:
{'img_size': 224, 'patch_size': 4, 'in_chans': 3, 'embed_dim': 96, 'depths': [2, 2, 6, 2], 'num_heads': [3, 6, 12, 24], 'window_size': 7, 'mlp_ratio': 4.0, 'drop_rate': 0.0, 'drop_path_rate': 0.3}
head param:
{'feat_dim': 1, 'num_class': 72778, 'is_am': 1, 'margin': 0.35, 'mv_weight': 1.12, 'scale': 32}
head param:
{'feat_dim': 1, 'num_class': 72778, 'is_am': 1, 'margin': 0.35, 'mv_weight': 1.12, 'scale': 32}
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
INFO 2022-05-14 05:32:18 train.py: 89] Epoch 0, iter 0/628, lr 0.000000, loss 0.733724
INFO 2022-05-14 05:32:18 distributed.py: 693] Reducer buckets have been rebuilt in this iteration.
INFO 2022-05-14 05:32:18 distributed.py: 693] Reducer buckets have been rebuilt in this iteration.
INFO 2022-05-14 05:36:00 train.py: 89] Epoch 0, iter 200/628, lr 0.000054, loss 0.425288
INFO 2022-05-14 05:39:42 train.py: 89] Epoch 0, iter 400/628, lr 0.000107, loss 0.390594
INFO 2022-05-14 05:43:24 train.py: 89] Epoch 0, iter 600/628, lr 0.000160, loss 0.385275
INFO 2022-05-14 05:43:54 train.py: 109] Save checkpoint Epoch_0.pt to disk...
INFO 2022-05-14 05:43:55 train.py: 89] Epoch 1, iter 0/628, lr 0.000167, loss 0.383455
INFO 2022-05-14 05:47:37 train.py: 89] Epoch 1, iter 200/628, lr 0.000220, loss 0.379838
INFO 2022-05-14 05:51:19 train.py: 89] Epoch 1, iter 400/628, lr 0.000273, loss 0.375310
INFO 2022-05-14 05:55:02 train.py: 89] Epoch 1, iter 600/628, lr 0.000326, loss 0.369841
INFO 2022-05-14 05:55:31 train.py: 109] Save checkpoint Epoch_1.pt to disk...
INFO 2022-05-14 05:55:32 train.py: 89] Epoch 2, iter 0/628, lr 0.000334, loss 0.367374
INFO 2022-05-14 05:59:14 train.py: 89] Epoch 2, iter 200/628, lr 0.000387, loss 0.363887
INFO 2022-05-14 06:02:56 train.py: 89] Epoch 2, iter 400/628, lr 0.000440, loss 0.359510
INFO 2022-05-14 06:06:37 train.py: 89] Epoch 2, iter 600/628, lr 0.000493, loss 0.354348
INFO 2022-05-14 06:07:07 train.py: 109] Save checkpoint Epoch_2.pt to disk...
INFO 2022-05-14 06:07:08 train.py: 89] Epoch 3, iter 0/628, lr 0.000467, loss 0.351159
INFO 2022-05-14 06:10:49 train.py: 89] Epoch 3, iter 200/628, lr 0.000460, loss 0.349335
INFO 2022-05-14 06:14:31 train.py: 89] Epoch 3, iter 400/628, lr 0.000452, loss 0.344647
INFO 2022-05-14 06:18:13 train.py: 89] Epoch 3, iter 600/628, lr 0.000443, loss 0.342461
INFO 2022-05-14 06:18:42 train.py: 109] Save checkpoint Epoch_3.pt to disk...
INFO 2022-05-14 06:18:43 train.py: 89] Epoch 4, iter 0/628, lr 0.000442, loss 0.340549
INFO 2022-05-14 06:22:25 train.py: 89] Epoch 4, iter 200/628, lr 0.000433, loss 0.339097
INFO 2022-05-14 06:26:07 train.py: 89] Epoch 4, iter 400/628, lr 0.000423, loss 0.339093
INFO 2022-05-14 06:29:49 train.py: 89] Epoch 4, iter 600/628, lr 0.000413, loss 0.336649
INFO 2022-05-14 06:30:18 train.py: 109] Save checkpoint Epoch_4.pt to disk...
INFO 2022-05-14 06:30:19 train.py: 89] Epoch 5, iter 0/628, lr 0.000412, loss 0.332489
INFO 2022-05-14 06:34:01 train.py: 89] Epoch 5, iter 200/628, lr 0.000401, loss 0.334267
INFO 2022-05-14 06:37:43 train.py: 89] Epoch 5, iter 400/628, lr 0.000390, loss 0.333708
INFO 2022-05-14 06:41:25 train.py: 89] Epoch 5, iter 600/628, lr 0.000378, loss 0.335756
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
INFO 2022-05-14 06:41:54 train.py: 109] Save checkpoint Epoch_5.pt to disk...
INFO 2022-05-14 06:41:55 train.py: 89] Epoch 6, iter 0/628, lr 0.000376, loss 0.332425
INFO 2022-05-14 06:45:37 train.py: 89] Epoch 6, iter 200/628, lr 0.000364, loss 0.331735
INFO 2022-05-14 06:49:19 train.py: 89] Epoch 6, iter 400/628, lr 0.000352, loss 0.329478
INFO 2022-05-14 06:53:01 train.py: 89] Epoch 6, iter 600/628, lr 0.000339, loss 0.328607
INFO 2022-05-14 06:53:30 train.py: 109] Save checkpoint Epoch_6.pt to disk...
INFO 2022-05-14 06:53:31 train.py: 89] Epoch 7, iter 0/628, lr 0.000337, loss 0.326724
INFO 2022-05-14 06:57:13 train.py: 89] Epoch 7, iter 200/628, lr 0.000324, loss 0.327748
INFO 2022-05-14 07:00:55 train.py: 89] Epoch 7, iter 400/628, lr 0.000311, loss 0.327110
INFO 2022-05-14 07:04:38 train.py: 89] Epoch 7, iter 600/628, lr 0.000297, loss 0.326411
INFO 2022-05-14 07:05:07 train.py: 109] Save checkpoint Epoch_7.pt to disk...
INFO 2022-05-14 07:05:08 train.py: 89] Epoch 8, iter 0/628, lr 0.000295, loss 0.325280
INFO 2022-05-14 07:08:50 train.py: 89] Epoch 8, iter 200/628, lr 0.000282, loss 0.325322
INFO 2022-05-14 07:12:31 train.py: 89] Epoch 8, iter 400/628, lr 0.000268, loss 0.324871
INFO 2022-05-14 07:16:12 train.py: 89] Epoch 8, iter 600/628, lr 0.000254, loss 0.324230
INFO 2022-05-14 07:16:42 train.py: 109] Save checkpoint Epoch_8.pt to disk...
INFO 2022-05-14 07:16:43 train.py: 89] Epoch 9, iter 0/628, lr 0.000253, loss 0.322755
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 65536.0
INFO 2022-05-14 07:20:25 train.py: 89] Epoch 9, iter 200/628, lr 0.000239, loss 0.323959
INFO 2022-05-14 07:24:06 train.py: 89] Epoch 9, iter 400/628, lr 0.000225, loss 0.326063
INFO 2022-05-14 07:27:47 train.py: 89] Epoch 9, iter 600/628, lr 0.000211, loss 0.324107
INFO 2022-05-14 07:28:17 train.py: 109] Save checkpoint Epoch_9.pt to disk...
INFO 2022-05-14 07:28:18 train.py: 89] Epoch 10, iter 0/628, lr 0.000210, loss 0.324139
INFO 2022-05-14 07:32:00 train.py: 89] Epoch 10, iter 200/628, lr 0.000196, loss 0.324567
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
INFO 2022-05-14 07:35:41 train.py: 89] Epoch 10, iter 400/628, lr 0.000183, loss 0.322253
INFO 2022-05-14 07:39:22 train.py: 89] Epoch 10, iter 600/628, lr 0.000170, loss 0.322852
INFO 2022-05-14 07:39:52 train.py: 109] Save checkpoint Epoch_10.pt to disk...
INFO 2022-05-14 07:39:53 train.py: 89] Epoch 11, iter 0/628, lr 0.000168, loss 0.321220
INFO 2022-05-14 07:43:34 train.py: 89] Epoch 11, iter 200/628, lr 0.000155, loss 0.322322
INFO 2022-05-14 07:47:16 train.py: 89] Epoch 11, iter 400/628, lr 0.000143, loss 0.321402
INFO 2022-05-14 07:50:57 train.py: 89] Epoch 11, iter 600/628, lr 0.000130, loss 0.321220
INFO 2022-05-14 07:51:26 train.py: 109] Save checkpoint Epoch_11.pt to disk...
INFO 2022-05-14 07:51:27 train.py: 89] Epoch 12, iter 0/628, lr 0.000129, loss 0.319084
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
INFO 2022-05-14 07:55:09 train.py: 89] Epoch 12, iter 200/628, lr 0.000117, loss 0.320553
INFO 2022-05-14 07:58:52 train.py: 89] Epoch 12, iter 400/628, lr 0.000106, loss 0.320433
INFO 2022-05-14 08:02:33 train.py: 89] Epoch 12, iter 600/628, lr 0.000095, loss 0.321681
INFO 2022-05-14 08:03:03 train.py: 109] Save checkpoint Epoch_12.pt to disk...
INFO 2022-05-14 08:03:04 train.py: 89] Epoch 13, iter 0/628, lr 0.000093, loss 0.320727
INFO 2022-05-14 08:06:45 train.py: 89] Epoch 13, iter 200/628, lr 0.000083, loss 0.321369
INFO 2022-05-14 08:10:27 train.py: 89] Epoch 13, iter 400/628, lr 0.000073, loss 0.320121
INFO 2022-05-14 08:14:09 train.py: 89] Epoch 13, iter 600/628, lr 0.000064, loss 0.319688
INFO 2022-05-14 08:14:38 train.py: 109] Save checkpoint Epoch_13.pt to disk...
INFO 2022-05-14 08:14:39 train.py: 89] Epoch 14, iter 0/628, lr 0.000063, loss 0.319798
INFO 2022-05-14 08:18:21 train.py: 89] Epoch 14, iter 200/628, lr 0.000054, loss 0.319298
INFO 2022-05-14 08:22:03 train.py: 89] Epoch 14, iter 400/628, lr 0.000046, loss 0.319395
INFO 2022-05-14 08:25:44 train.py: 89] Epoch 14, iter 600/628, lr 0.000039, loss 0.319412
INFO 2022-05-14 08:26:14 train.py: 109] Save checkpoint Epoch_14.pt to disk...
INFO 2022-05-14 08:26:15 train.py: 89] Epoch 15, iter 0/628, lr 0.000038, loss 0.317520
INFO 2022-05-14 08:29:56 train.py: 89] Epoch 15, iter 200/628, lr 0.000032, loss 0.319238
INFO 2022-05-14 08:33:37 train.py: 89] Epoch 15, iter 400/628, lr 0.000026, loss 0.318891
INFO 2022-05-14 08:37:20 train.py: 89] Epoch 15, iter 600/628, lr 0.000021, loss 0.319401
INFO 2022-05-14 08:37:49 train.py: 109] Save checkpoint Epoch_15.pt to disk...
INFO 2022-05-14 08:37:50 train.py: 89] Epoch 16, iter 0/628, lr 0.000020, loss 0.317400
INFO 2022-05-14 08:41:32 train.py: 89] Epoch 16, iter 200/628, lr 0.000016, loss 0.318756
INFO 2022-05-14 08:45:14 train.py: 89] Epoch 16, iter 400/628, lr 0.000012, loss 0.318693
INFO 2022-05-14 08:48:55 train.py: 89] Epoch 16, iter 600/628, lr 0.000009, loss 0.318702
INFO 2022-05-14 08:49:25 train.py: 109] Save checkpoint Epoch_16.pt to disk...
INFO 2022-05-14 08:49:26 train.py: 89] Epoch 17, iter 0/628, lr 0.000009, loss 0.317516
INFO 2022-05-14 08:53:08 train.py: 89] Epoch 17, iter 200/628, lr 0.000007, loss 0.318854
INFO 2022-05-14 08:56:50 train.py: 89] Epoch 17, iter 400/628, lr 0.000005, loss 0.318657
INFO 2022-05-14 09:00:32 train.py: 89] Epoch 17, iter 600/628, lr 0.000005, loss 0.318824
INFO 2022-05-14 09:01:02 train.py: 109] Save checkpoint Epoch_17.pt to disk...
